\documentclass{article}

\usepackage{hyperref}
\usepackage{parskip}
\usepackage{listings}
\usepackage{amsmath}

%%
%% GLSL definition (c) 2020 Benno Bielmeier
%%
\lstdefinelanguage{GLSL}%
{%
	morekeywords={%
	% HLSL constants
		false,FALSE,NULL,true,TRUE,%
	% GLSL predefinde macro constant
		__LINE__,__FILE__,__VERSION__,GL_core_profile,GL_es_profile,GL_compatibility_profile,%
	% GLSL precision modifier
		precision,highp,mediump,lowp,%
	% GLSL control keywords
		break,case,continue,default,discard,do,else,for,if,return,switch,while,%
	% GLSL types
		void,bool,int,uint,float,double,vec2,vec3,vec4,dvec2,dvec3,dvec4,bvec2,bvec3,bvec4,ivec2,ivec3,ivec4,uvec2,uvec3,uvec4,mat2,mat3,mat4,mat2x2,mat2x3,mat2x4,mat3x2,mat3x3,mat3x4,mat4x2,mat4x3,mat4x4,dmat2,dmat3,dmat4,dmat2x2,dmat2x3,dmat2x4,dmat3x2,dmat3x3,dmat3x4,dmat4x2,dmat4x3,dmat4x4,sampler1D,sampler2D,sampler3D,image1D,image2D,image3D,samplerCube,imageCube,sampler2DRect,image2DRect,sampler1DArray,sampler2DArray,image1DArray,image2DArray,samplerBuffer,imageBuffer,sampler2DMS,image2DMS,sampler2DMSArray,image2DMSArray,samplerCubeArray,imageCubeArray,sampler1DShadow,sampler2DShadow,sampler2DRectShadow,sampler1DArrayShadow,sampler2DArrayShadow,samplerCubeShadow,samplerCubeArrayShadow,isampler1D,isampler2D,isampler3D,iimage1D,iimage2D,iimage3D,isamplerCube,iimageCube,isampler2DRect,iimage2DRect,isampler1DArray,isampler2DArray,iimage1DArray,iimage2DArray,isamplerBuffer,iimageBuffer,isampler2DMS,iimage2DMS,isampler2DMSArray,iimage2DMSArray,isamplerCubeArray,iimageCubeArray,atomic_uint,usampler1D,usampler2D,usampler3D,uimage1D,uimage2D,uimage3D,usamplerCube,uimageCube,usampler2DRect,uimage2DRect,usampler1DArray,usampler2DArray,uimage1DArray,uimage2DArray,usamplerBuffer,uimageBuffer,usampler2DMS,uimage2DMS,usampler2DMSArray,uimage2DMSArray,usamplerCubeArray,uimageCubeArray,struct,%
	% GLSL support variables
		gl_BackColor,gl_BackLightModelProduct,gl_BackLightProduct,gl_BackMaterial,gl_BackSecondaryColor,gl_ClipDistance,gl_ClipPlane,gl_ClipVertex,gl_Color,gl_DepthRange,gl_DepthRangeParameters,gl_EyePlaneQ,gl_EyePlaneR,gl_EyePlaneS,gl_EyePlaneT,gl_Fog,gl_FogCoord,gl_FogFragCoord,gl_FogParameters,gl_FragColor,gl_FragCoord,gl_FragData,gl_FragDepth,gl_FrontColor,gl_FrontFacing,gl_FrontLightModelProduct,gl_FrontLightProduct,gl_FrontMaterial,gl_FrontSecondaryColor,gl_InstanceID,gl_Layer,gl_LightModel,gl_LightModelParameters,gl_LightModelProducts,gl_LightProducts,gl_LightSource,gl_LightSourceParameters,gl_MaterialParameters,gl_ModelViewMatrix,gl_ModelViewMatrixInverse,gl_ModelViewMatrixInverseTranspose,gl_ModelViewMatrixTranspose,gl_ModelViewProjectionMatrix,gl_ModelViewProjectionMatrixInverse,gl_ModelViewProjectionMatrixInverseTranspose,gl_ModelViewProjectionMatrixTranspose,gl_MultiTexCoord0,gl_MultiTexCoord1,gl_MultiTexCoord2,gl_MultiTexCoord3,gl_MultiTexCoord4,gl_MultiTexCoord5,gl_MultiTexCoord6,gl_MultiTexCoord7,gl_Normal,gl_NormalMatrix,gl_NormalScale,gl_ObjectPlaneQ,gl_ObjectPlaneR,gl_ObjectPlaneS,gl_ObjectPlaneT,gl_Point,gl_PointCoord,gl_PointParameters,gl_PointSize,gl_Position,gl_PrimitiveIDIn,gl_ProjectionMatrix,gl_ProjectionMatrixInverse,gl_ProjectionMatrixInverseTranspose,gl_ProjectionMatrixTranspose,gl_SecondaryColor,gl_TexCoord,gl_TextureEnvColor,gl_TextureMatrix,gl_TextureMatrixInverse,gl_TextureMatrixInverseTranspose,gl_TextureMatrixTranspose,gl_Vertex,gl_VertexID,%
	% GLSL support constants
		gl_MaxClipPlanes,gl_MaxCombinedTextureImageUnits,gl_MaxDrawBuffers,gl_MaxFragmentUniformComponents,gl_MaxLights,gl_MaxTextureCoords,gl_MaxTextureImageUnits,gl_MaxTextureUnits,gl_MaxVaryingFloats,gl_MaxVertexAttribs,gl_MaxVertexTextureImageUnits,gl_MaxVertexUniformComponents,%
	% GLSL support functions
		abs,acos,all,any,asin,atan,ceil,clamp,cos,cross,degrees,dFdx,dFdy,distance,dot,equal,exp,exp2,faceforward,floor,fract,ftransform,fwidth,greaterThan,greaterThanEqual,inversesqrt,length,lessThan,lessThanEqual,log,log2,matrixCompMult,max,min,mix,mod,noise1,noise2,noise3,noise4,normalize,not,notEqual,outerProduct,pow,radians,reflect,refract,shadow1D,shadow1DLod,shadow1DProj,shadow1DProjLod,shadow2D,shadow2DLod,shadow2DProj,shadow2DProjLod,sign,sin,smoothstep,sqrt,step,tan,texture1D,texture1DLod,texture1DProj,texture1DProjLod,texture2D,texture2DLod,texture2DProj,texture2DProjLod,texture3D,texture3DLod,texture3DProj,texture3DProjLod,textureCube,textureCubeLod,transpose,%
	% GLSL struct member -> FixMe: Should have dot(.) as delimiter
		rgb
	},
	sensitive=true,%
	morecomment=[s]{/*}{*/},%
	morecomment=[l]//,%
	morestring=[b]",%
	morestring=[b]',%
	moredelim=*[directive]\#,%
	% keyword.control.hlsl
	moredirectives={define,defined,elif,else,if,ifdef,endif,line,error,ifndef,include,pragma,undef,warning,extension,version}%
}[keywords,comments,strings,directives]%

\lstset{language=GLSL}

\title{HBAO}
\author{Gerard Martin Teixidor}

% TODO REFERENCES

\begin{document}
\maketitle

\section*{Geometry pass}
Since we are only computing ambient occlusion, the only information we need to store in the G Buffer are the face normals and the depth. There is no need to store the fragment positions or any material properties. Both, normals and depth, are going to be saved in view space.

To correctly implement SSAO, we need to retrieve the face normals instead of the vertex normals. To do so, we use the dFdx and dFdy GLSL instructions which compute the partial derivative of the fragment position. Given these two derivatives, we compute the normal vector using the cross product.

\begin{lstlisting}
vec3 normal_view = normalize(cross(dFdx(pos_view),
                                   dFdy(pos_view)));
\end{lstlisting}

The depth of each fragment is computed the same way OpenGL computes the Z Buffer. This means that the depth, once in view space, is multiplied by the projection matrix and divided by the fourth component. Once the depth has been divided, we have to take into consideration that it is no longer lineal. In addition, because of this division now the depth value goes from -1 to 1. Finally, the depth is normalized to the range 0 to 1, although this step is not really necessary.

\begin{lstlisting}
gl_Position = proj * view * model * vec4(vert, 1.0);
depth_view = (gl_Position.z / gl_Position.w) * 0.5 + 0.5;
\end{lstlisting}

All this data is being packed in a single RGBA texture, using 32 bits per channel. The normal is stored in the RGB channels while the depth is stored in the alpha channel. The use of 32 bits is needed in the alpha channel to avoid z-fighting problems. Since the normals does not require that much precision, a further improvement could be to use two textures, one RGB texture for the normals using 8 or 16 bits, and a single 32 bits channel texture for the depth. This would reduce the space required for the G Buffer reducing the memory bandwidth.

\subsection*{Retrieving position from depth}
Once we are doing the light pass, we might want to retrieve a fragment position from the G Buffer. This can be done by unprojecting the depth previously stored in the alpha channel of the G Buffer. Considering the retrieved depth  at texture coordinates $s$, $t$, the OpenGL projection matrix $P$, the field of view $f$  and the aspect ratio $a$:

\begin{align*}
x_{view} = & s * tan(\frac{f}{2}) * a * -z_{view} \\
y_{view} = & t * tan(\frac{f}{2}) * -z_{view} \\
z_{view} = & \frac{P_{4,3}}{2d -1 + P_{3,3}}
\end{align*}

As a reference, we used the post SSAO With Depth Reconstruction~\cite{depth}. Take into consideration that the matrix of this post is different from the one used in this project. The projection matrix used in this project is the one provided by the function glm::perspective of the glm library.


To reduce the noise produced by some SSAO techniques, we can apply a post processing blur. This blur is a simple implementation of a Gaussian blur with the optimization of doing two passes, one for each axis, instead of a single one. This two passes implementation allows to reduce the time complexity from $O(n^2)$ to $O(n)$.

To blurrier image, we perform multiple passes of the same kernel of size $n=5$. We could implement a better solution by using a single bigger kernel instead of multiple passes of a small kernel. This can be performed since a single blur radius is equivalent to the square root of the sum of the squares of the individual blur radius.

$$
r = \sqrt{r^2_a + r^2_b}
$$

\section*{HBAO}
The main idea of Horizon Based Ambient Occlusion is to scan the geometry around the shading point to find the horizon vector. Once this vector has been found, and knowing the point tangent vector, we can compute the ambient occlusion.

This implementation of the HBAO tries to be as accurate as possible to the original version. To do so, we used as a reference the published paper in conjunction with presentation slides~\cite{hbao}. These slides are interesting since many details are not present in the paper.

\subsection*{Implemented features}
This section lists the original HBAO features which have been implemented.

\paragraph*{Uniform directions distribution} At each fragment, to compute the ambient occlusion sample, we sample 4 orthogonal directions. If we want more direction samples, we will use multiple orthogonal directions, making the sample directions alway a multiple of 4. Using these 4 orthogonal directions, in conjunction with per pixel randomization, allows to more uniformly distribute the depth samples.
 
Per pixel randomization. To avoid axis aligned artifacts, randomization is introduced during the sampling process:
\begin{itemize}
\item The 4 orthogonal directions are rotated randomly. The rotation is between 0 and 90 degrees since rotation more than a quarter of a circumference would produce the same result.
\item Sampling along the direction ray is jittered by randomizing the step size. Between 0.1 and 1.0 of its original size.
\end{itemize}
To improve the uniformity of the noise, we used a blue noise texture from the blog post Free blue noise textures~\cite{noise}.

\paragraph*{Face normals} During the geometric pass, instead of computing the normal per vertex and interpolate them, we compute the normals per face. A more detailed explanation has been described in the previous Geometry pass section.

\paragraph*{Tangent angle bias} Due to the low tessellation of some geometry, to avoid false occlusions, we add a constant bias to the tangent angle. This will remove some correct ambient occlusion but will hide the aliasing effect of low polygonal geometry.

\paragraph*{Per-sample attenuation} Due to the limited radius, to avoid a hard contrast between the fragments with ambient occlusion and the ones without, a per-sample attenuation is applied. This attenuation weights the samples to be less important as we take more samples along the sampling direction. 

\paragraph*{Snapping samples} Because the sampling along the sampling direction is done in view space, when we project the sampling view space fragment to texture space we can not guarantee the texture space fragment to be aligned with the texel of the G Buffer. Because of that, we have to snap the fragment to the nearest texel center and unproject the fragment to get the new view space fragment. This will guarantee that the depth from the G Buffer that we are using really corresponds to the depth of the new fragment in view space.

The only missing feature with respect to the original HBAO is the use of depth-dependent Gaussian blur. Instead, this implementation only uses a simple Gaussian blur.

\bibliographystyle{unsrt}
\bibliography{report}

\end{document}